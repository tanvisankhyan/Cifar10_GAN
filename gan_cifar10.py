# -*- coding: utf-8 -*-
"""GAN_Cifar10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxtqqyX4wU6YLc-xL4dmbZQljPZZ9Ph6
"""



from numpy import zeros
from numpy import ones
from numpy.random import randn
from numpy.random import randint
from keras.datasets.cifar10 import load_data
from tensorflow.keras.datasets import cifar10
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout

from matplotlib import pyplot as plt

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# plot 25 images
for i in range(25):
	plt.subplot(5, 5, 1 + i)
	plt.axis('off')
	plt.imshow(X_train[i])
plt.show()

print('Training images: {}'.format(X_train.shape))
print('Training images: {}'.format(y_train.shape))
print('Testing images: {}'.format(X_test.shape))
print('Testing images: {}'.format(y_test.shape))

def define_discriminator(in_shape=(32,32,3)):
	model = Sequential()
	
	model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape)) #16x16x128
	model.add(LeakyReLU(alpha=0.2))
	
	model.add(Conv2D(128, (3,3), strides=(2,2), padding='same')) #8x8x128
	model.add(LeakyReLU(alpha=0.2))
	
	model.add(Flatten()) #shape of 8192
	model.add(Dropout(0.4))
	model.add(Dense(1, activation='sigmoid')) #shape of 1
	# compile model
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
	return model

test_discr = define_discriminator()
print(test_discr.summary())

def define_generator(latent_dim):   
	model = Sequential()
	n_nodes = 128 * 8 * 8  #8192 nodes
	model.add(Dense(n_nodes, input_dim=latent_dim)) 
	model.add(LeakyReLU(alpha=0.2))
	model.add(Reshape((8, 8, 128)))   
	# upsample to 16x16
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) #16x16x128
	model.add(LeakyReLU(alpha=0.2))
	# upsample to 32x32
	model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')) #32x32x128
	model.add(LeakyReLU(alpha=0.2))
	# generate
	model.add(Conv2D(3, (8,8), activation='tanh', padding='same')) #32x32x3
	return model  

test_gen = define_generator(100)
print(test_gen.summary())

def define_gan(generator, discriminator):
	discriminator.trainable = False 
	model = Sequential()
	model.add(generator)
	model.add(discriminator)
	opt = Adam(lr=0.0002, beta_1=0.5)
	model.compile(loss='binary_crossentropy', optimizer=opt)
	return model

def load_real_samples():
	(trainX, _), (_, _) = load_data()
	X = trainX.astype('float32')
	X = (X - 127.5) / 127.5 
	return X

def generate_real_samples(dataset, n_samples):
	ix = randint(0, dataset.shape[0], n_samples)
	X = dataset[ix]
	y = ones((n_samples, 1)) 
	return X, y

def generate_latent_points(latent_dim, n_samples):
	x_input = randn(latent_dim * n_samples)
	x_input = x_input.reshape(n_samples, latent_dim)
	return x_input

def generate_fake_samples(generator, latent_dim, n_samples):
	x_input = generate_latent_points(latent_dim, n_samples)
	X = generator.predict(x_input)
	y = zeros((n_samples, 1)) 
	return X, y

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=50, n_batch=128):
	bat_per_epo = int(dataset.shape[0] / n_batch)
	half_batch = int(n_batch / 2) 
	for i in range(n_epochs):
		for j in range(bat_per_epo):
			X_real, y_real = generate_real_samples(dataset, half_batch)
			d_loss_real, _ = d_model.train_on_batch(X_real, y_real) 
			X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
			d_loss_fake, _ = d_model.train_on_batch(X_fake, y_fake)
			X_gan = generate_latent_points(latent_dim, n_batch)
			y_gan = ones((n_batch, 1))     
			g_loss = gan_model.train_on_batch(X_gan, y_gan)
			print('Epoch>%d, Batch %d/%d, d1=%.3f, d2=%.3f g=%.3f' %
				(i+1, j+1, bat_per_epo, d_loss_real, d_loss_fake, g_loss))
	g_model.save('cifar_generator_50epochs.h5')

latent_dim = 100
discriminator = define_discriminator()
generator = define_generator(latent_dim)
gan_model = define_gan(generator, discriminator)
dataset = load_real_samples()
train(generator, discriminator, gan_model, dataset, latent_dim, n_epochs=50)

pwd

ls

from keras.models import load_model
from numpy.random import randn


# Plot generated images 
def show_plot(examples, n):
	for i in range(n * n):
		plt.subplot(n, n, 1 + i)
		plt.axis('off')
		plt.imshow(examples[i, :, :, :])
	plt.show()

# load model
model = load_model('cifar_generator_50epochs.h5') #Model trained for 100 epochs
# generate images
latent_points = generate_latent_points(100, 50000)  #Latent dim and n_samples
# generate images
X = model.predict(latent_points)
# scale from [-1,1] to [0,1]
X = (X + 1) / 2.0

import numpy as np
X = (X*255).astype(np.uint8)

# plot the result
show_plot(X, 4)

#Note: CIFAR10 classes are: airplane, automobile, bird, cat, deer, dog, frog, horse,
# ship, truck

X.shape

X_train.shape

print('Training images: {}'.format(X_train.shape))
print('Training images: {}'.format(y_train.shape))
print('Testing images: {}'.format(X_test.shape))

print('Testing images: {}'.format(y_test.shape))

X_train=np.concatenate([X,X_train])

print('Training images: {}'.format(X_train.shape))
print('Training images: {}'.format(y_train.shape))
print('Testing images: {}'.format(X_test.shape))

print('Testing images: {}'.format(y_test.shape))

X_train.shape



print(y_train[:5])
y_train = y_train.reshape(-1,)
print(y_train[:5])

y_test = y_test.reshape(-1,)

dataset_class = ["airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck"]

def plot_image(X, y, i):
    plt.figure(figsize = (20,2))
    plt.imshow(X[i])
    plt.xlabel(dataset_class[y[i]])
plot_image(X_train, y_train, 5)

#Normalization
X_train = X_train / 255.0
X_test = X_test / 255.0

import matplotlib.pyplot as plt
from keras.utils import np_utils
from tensorflow.keras.datasets import cifar10
from tensorflow.keras import datasets, layers, models
cnnmodel = models.Sequential([
    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])
cnnmodel.summary()

# with defaul adam learning rate = 0.001
cnnmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

import time
t1 = time.time()
history = cnnmodel.fit(X_train, y_train, epochs=100)
t2 = time.time()
print('\nTime: {} min'.format((t2-t1)/60))

#train set accuracy
cnnmodel.evaluate(X_train,y_train)

#test set accuracy
cnnmodel.evaluate(X_test,y_test)

from sklearn.metrics import confusion_matrix , classification_report
import numpy as np
y_pred1 = cnnmodel.predict(X_test)
y_pred_classes = [np.argmax(element) for element in y_pred1]

print("Classification Report: \n", classification_report(y_test, y_pred_classes))

import pandas as pd
pd.DataFrame(history.history).plot(figsize=(11, 8))
plt.grid(True)
plt.gca().set_ylim(0, 1)
plt.show()

